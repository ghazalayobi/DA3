---
title: "Data Analysis 3 : Assignment II : Technichal Report"
author: "Ghazal Ayobi"
geometry : margin=1.7cm
fontsize: 9pt
output: 
  pdf_document:
  extra_dependencies: ["flafter"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r message=FALSE, warning=FALSE, include=FALSE}
# Clearing the environment
rm(list=ls())
# Loading the Libraries
library(tidyverse)
library(caret)
library(modelsummary)
library(stargazer)
library(xtable)
library(rattle)
library(kableExtra)
library(data.table)
library(ggplot2)
library(GGally)
library(gridExtra)
library(knitr)
library(viridis)
library(directlabels)
library(Hmisc)
library(cowplot)
library(ranger)
library(glmnet)
library(grid)
library(skimr)
library(gbm)
library(fixest)
library(rpart)
library(rpart.plot)

```



```{r message=FALSE, warning=FALSE, include=FALSE}

getwd()

setwd("/Users/ghazalayobi/DA3/A2")
path <- "/Users/ghazalayobi/DA3/A2"

#location folders
data_in  <- paste0(path,"/data/raw/")
data_out <- paste0(path,"/data/clean/")
output <- paste0(path, "/output/")

dfx <- read_csv("https://raw.githubusercontent.com/ghazalayobi/DA3/main/A2/data/clean/airbnb_ny_cleaned.csv")
# github link for clean data()
df <- subset(dfx, select = -c(latitude, longitude))

source("https://raw.githubusercontent.com/ghazalayobi/DA3/main/da_helper_functions.R")
source("https://raw.githubusercontent.com/ghazalayobi/DA3/main/theme_bg.R")

# checking for missing variables
to_filter <- sort(sapply(df, function(x) sum(is.na(x)/nrow(df)*100)))
to_filter[to_filter > 0]

# quick look at data
#glimpse(df)
#skim(df)

```



```{r message=FALSE, warning=FALSE, include=FALSE}
# Checking for data summaries
# Accommodates Summary
sum_accomm <- datasummary(price* (`Accommodates` = as.factor(n_accommodates)) ~ N + min + max + 
                            Percent() + mean, data = df, title = "Accommodates Summary")

# Number of Beds Summary
sum_beds <- datasummary(price* (`Beds` = as.factor(n_beds)) ~ N + min + max + 
                            Percent() + mean, data = df, title = "Beds Summary")

# Number of Beds Summary
sum_bedrooms <- datasummary(price* (`Bedrooms` = as.factor(n_bedrooms)) ~ N + min + max + 
                            Percent() + mean, data = df, title = "Bedrooms Summary")

# Room type Summary
sum_room_type <- datasummary(price* (`Room Type` = as.factor(f_room_type)) ~ N + min + max + 
                               Percent() + mean, data = df, title = "Room Type Summary")

# Property Type Summary
sum_property_type <- datasummary(price* (`Property Type` = as.factor(f_property_type)) ~ N + min + max + 
                                   Percent() + mean, data = df, title = "Property Type Summary")

# Host response time Summary
sum_host_res_time <- datasummary(price* (`Room Type` = as.factor(f_host_response_time)) ~ N + min + max + 
                                   Percent() + mean, data = df, title = "Host Response Time Summary")

# Minimum Nights summary
sum_nnights <- datasummary((`Min Nights` = n_minimum_nights) ~ N + min + 
                             max + mean, data = df, title = "Minimum nights Summary")


# Neighborhood group summary
sum_neighbourhood_group <- datasummary((`Neighbourhood Group` = neighbourhood_group_cleansed) ~ N + min + 
                             max + mean, data = df, title = "Neighbourhood Group Summary")
```

```{r message=FALSE, warning=FALSE, include=FALSE}
# Histograms

# price -> skewed distribution with long right tail
hist_price <- ggplot(data=df, aes(x=price)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)), binwidth = 10, boundary=0,
                 color = "white", fill = "#440154", alpha = 0.7) +
  coord_cartesian(xlim = c(0, 500)) +
  labs(x = "Price (US dollars)",y = "Percent")+
  scale_y_continuous(expand = c(0.00,0.00),limits=c(0, 0.10), breaks = seq(0, 0.10, by = 0.03), labels = scales::percent_format(1)) +
  scale_x_continuous(expand = c(0.00,0.00),limits=c(0,500), breaks = seq(0,500, 50)) +
  theme_bw() 
hist_price

save_fig("fig1-hist-price", output, "small")

# lnprice -> closer to a normal distribution
hist_ln_price <- ggplot(data=df, aes(x=ln_price)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)), binwidth = 0.15,
                 color = "white", fill = "#440154", alpha = 0.7) +
  coord_cartesian(xlim = c(3.5, 6.5)) +
  scale_y_continuous(expand = c(0.00,0.00),limits=c(0, 0.15), breaks = seq(0, 0.10, by = 0.05), labels = scales::percent_format(5L)) +
  labs(x = "ln(price, US dollars)",y = "Percent")+
  theme_bw() 
hist_ln_price

save_fig("fig2-hist-ln-price", output, "small")

```


```{r message=FALSE, warning=FALSE, include=FALSE}
# Box plots and jitter for one variable

# Boxplot for the number of accommodates
jitter_accomm <- ggplot(df,aes(factor(n_accommodates), price, color = "#440154" )) + 
  geom_boxplot(alpha = 0.1, frame = FALSE) + 
  geom_jitter(height = 0, width = 0.1, alpha = 0.2) +
  scale_color_viridis(option = "D", discrete = TRUE)+
  scale_fill_viridis(option = "D", discrete = TRUE) +
  theme_bw() +
  theme(legend.position = "none", panel.border = element_blank(), axis.text=element_text(size=8), plot.title = element_text(size = 12L, face = "bold", hjust = 0.5) ) +
  labs(x = "Nr of Accommodates", y = "Price") +
  ggtitle("Nr of Accommodates ")
jitter_accomm

save_fig("fig3-boxplot-accommodates", output, "small")

# Boxplot for the Property type
jitter_property_type <- ggplot(df,aes(factor(f_property_type), price, color = "#440154" )) + 
  geom_boxplot(alpha = 0.1, frame = FALSE) + 
  geom_jitter(height = 0, width = 0.1, alpha = 0.2) +
  scale_color_viridis(option = "D", discrete = TRUE)+
  scale_fill_viridis(option = "D", discrete = TRUE) +
  theme_bw() +
  theme(legend.position = "none", panel.border = element_blank(), axis.text=element_text(size=8), plot.title = element_text(size = 12L, face = "bold", hjust = 0.5) ) +
  labs(x = "Property Type", y = "Price") +
  ggtitle("Property Type ")
jitter_property_type

save_fig("fig4-boxplot-prtype", output, "small")

# Boxplot for the room type
jitter_room_type <- ggplot(df,aes(factor(f_room_type), price, color = "#440154" )) + 
  geom_boxplot(alpha = 0.1, frame = FALSE) + 
  geom_jitter(height = 0, width = 0.1, alpha = 0.2) +
  scale_color_viridis(option = "D", discrete = TRUE)+
  scale_fill_viridis(option = "D", discrete = TRUE) +
  theme_bw() +
  theme(legend.position = "none", panel.border = element_blank(), axis.text=element_text(size=8), plot.title = element_text(size = 12L, face = "bold", hjust = 0.5) ) +
  labs(x = "Room Type", y = "Price") +
  ggtitle("Room Type ")
jitter_room_type

save_fig("fig5-boxplot-rmtype", output, "small")

# Boxplot for the Host response time
jitter_host_res_time <- ggplot(df,aes(factor(f_host_response_time), price, color = "#440154" )) + 
  geom_boxplot(alpha = 0.1, frame = FALSE) + 
  geom_jitter(height = 0, width = 0.1, alpha = 0.2) +
  scale_color_viridis(option = "D", discrete = TRUE)+
  scale_fill_viridis(option = "D", discrete = TRUE) +
  theme_bw() +
  theme(legend.position = "none", panel.border = element_blank(), axis.text=element_text(size=8), plot.title = element_text(size = 12L, face = "bold", hjust = 0.5) ) +
  labs(x = "Host response time", y = "Price") +
  ggtitle("Host Response Time ")
jitter_host_res_time

save_fig("fig6-boxplot-host-response", output, "small")

# boxplot for the grouped neighborhood
jitter_neighbourhood_group <- ggplot(df,aes(factor(f_neighbourhood_group_cleansed), price, color = "#440154" )) + 
  geom_boxplot(alpha = 0.1, frame = FALSE) + 
  geom_jitter(height = 0, width = 0.1, alpha = 0.2) +
  scale_color_viridis(option = "D", discrete = TRUE)+
  scale_fill_viridis(option = "D", discrete = TRUE) +
  theme_bw() +
  theme(legend.position = "none", panel.border = element_blank(), axis.text=element_text(size=8), plot.title = element_text(size = 12L, face = "bold", hjust = 0.5) ) +
  labs(x = "Neighbourhood group", y = "Price") +
  ggtitle("Neighbourhood Group")
jitter_neighbourhood_group

save_fig("fig7-boxplot-neighbourhood", output, "small")

# Boxplot for the number of bathrooms
jitter_bathrooms <- ggplot(df,aes(factor(n_bathrooms), price, color = "#440154" )) + 
  geom_boxplot(alpha = 0.1, frame = FALSE) + 
  geom_jitter(height = 0, width = 0.1, alpha = 0.2) +
  scale_color_viridis(option = "D", discrete = TRUE)+
  scale_fill_viridis(option = "D", discrete = TRUE) +
  theme_bw() +
  theme(legend.position = "none", panel.border = element_blank(), axis.text=element_text(size=8), plot.title = element_text(size = 12L, face = "bold", hjust = 0.5) ) +
  labs(x = "Nr Bathrooms", y = "Price") +
  ggtitle("Number of Bathrooms Price Distribution")
jitter_bathrooms

save_fig("fig8-boxplot-bathroom", output, "small")

# Boxplot for the number of bedrooms
jitter_bedrooms <- ggplot(df,aes(factor(n_bedrooms), price, color = "#440154" )) + 
  geom_boxplot(alpha = 0.1, frame = FALSE) + 
  geom_jitter(height = 0, width = 0.1, alpha = 0.2) +
  scale_color_viridis(option = "D", discrete = TRUE)+
  scale_fill_viridis(option = "D", discrete = TRUE) +
  theme_bw() +
  theme(legend.position = "none", panel.border = element_blank(), axis.text=element_text(size=8), plot.title = element_text(size = 12L, face = "bold", hjust = 0.5) ) +
  labs(x = "Nr bedrooms", y = "Price") +
  ggtitle("Number of Bedrooms Price Distribution")
jitter_bedrooms
save_fig("fig9-boxplot-bedroom", output, "small")

# Boxplot for the number of beds
jitter_beds <- ggplot(df,aes(factor(n_beds), price, color = "#440154" )) + 
  geom_boxplot(alpha = 0.1, frame = FALSE) + 
  geom_jitter(height = 0, width = 0.1, alpha = 0.2) +
  scale_color_viridis(option = "D", discrete = TRUE)+
  scale_fill_viridis(option = "D", discrete = TRUE) +
  theme_bw() +
  theme(legend.position = "none", panel.border = element_blank(), axis.text=element_text(size=8), plot.title = element_text(size = 12L, face = "bold", hjust = 0.5) ) +
  labs(x = "Nr beds", y = "Price") +
  ggtitle("Number of Beds Price Distribution")
jitter_beds

save_fig("fig10-boxplot-beds", output, "small")
```


```{r message=FALSE, warning=FALSE, include=FALSE}
# Two variables boxplots

# property type and kitchen price distribution
boxplot_pr_kitchen <- ggplot(df, aes(x = factor(property_type), y = price, fill = factor(d_kitchen), color=factor(d_kitchen))) +
  geom_boxplot(alpha=0.6, na.rm=T, outlier.shape = NA, width = 0.8) +
  stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
  scale_color_viridis(option = "D", discrete = TRUE) +
  scale_fill_viridis(option = "D", discrete = TRUE) +
  labs(x = "Property Type",y = "Price", color = "Kitchen", fill = "Kitchen") + 
  theme_bw() +
  theme(legend.position = "top", panel.border = element_blank(), axis.text=element_text(size=8), plot.title = element_text(size = 12L, face = "bold", hjust = 0.5) ) +
  ggtitle("Property type and Kitchen price distribution")
boxplot_pr_kitchen
save_fig("fig11-boxplots-prtype-kitchen", output, "small")

# property type and and accommodates
boxplot_accomm_pr <- ggplot(df, aes(x = factor(n_accommodates), y = price, fill = factor(f_property_type), color=factor(f_property_type))) +
  geom_boxplot(alpha=0.6, na.rm=T, outlier.shape = NA, width = 0.8) +
  stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
  scale_color_viridis(option = "D", discrete = TRUE) +
  scale_fill_viridis(option = "D", discrete = TRUE) +
  labs(x = "Accommodates (Person)",y = "Price (USD Dollar)", color = "Property Type", fill = "Property Type") + 
  theme_bw() +
  theme(legend.position = "top", panel.border = element_blank(), axis.text=element_text(size=8), plot.title = element_text(size = 12L, face = "bold", hjust = 0.5) ) +
  ggtitle("Property type and accommodates price distribution")

boxplot_accomm_pr
save_fig("fig12-boxplots-prtype-accommodates", output, "small")

```



```{r message=FALSE, warning=FALSE, include=FALSE}

# Part II 
#------------------------------------------
# Defining variables

# define variables
basic_lev <- c("n_accommodates", "f_property_type", "n_beds", "n_days_since", "flag_days_since")

# Factorized variables
basic_add <- c("f_bathroom", "f_neighbourhood_group_cleansed", "f_host_response_time")

# reviews
reviews <- c("f_number_of_reviews", "n_review_scores_rating", "n_reviews_per_month")

# higher orders
poly_lev <- c("n_accommodates2", "n_days_since2", "n_days_since3")

# Dummy Variables 
dummies <- grep("^d_.*", names(df), value = TRUE)


```



```{r message=FALSE, warning=FALSE, include=FALSE}
#Look up room type interactions

p1 <- price_diff_by_variables2(df, "f_property_type", "d_air_conditioning", "Property Type", "Air Conditioning")
p2 <- price_diff_by_variables2(df, "f_property_type", "d_elevator", "Property Type", "Elevator")
p3 <- price_diff_by_variables2(df, "f_property_type", "d_kitchen", "Property Type", "Kitchen")
p4 <- price_diff_by_variables2(df, "f_property_type", "d_essentials", "Property Type", "Essential")
p5 <- price_diff_by_variables2(df, "f_property_type", "d_pool", "Property Type", "Pool")
p6 <- price_diff_by_variables2(df, "f_property_type", "d_washer", "Property Type", "Washer")
p7 <- price_diff_by_variables2(df, "f_property_type", "d_wifi", "Property Type", "Wifi")
p8 <- price_diff_by_variables2(df, "f_property_type", "d_tv", "Property Type", "TV")
p9 <- price_diff_by_variables2(df, "f_property_type", "d_patio_or_balcony", "Property Type", "Balcony")
p10 <- price_diff_by_variables2(df, "f_property_type", "d_gym", "Property Type", "Gym")
p11 <- price_diff_by_variables2(df, "f_property_type", "d_dryer", "Property Type", "Dryer")
p12 <- price_diff_by_variables2(df, "f_property_type", "d_stove", "Property Type", "Stove")



g_interactions1 <- plot_grid(p1, p2, p3, p4, p5, p6, nrow=3, ncol=2) 
g_interactions2 <- plot_grid(p7, p8, p9, p10, p11, p12, nrow=3, ncol=2)



save_fig("fig13-interactions", output, "small")
```


```{r message=FALSE, warning=FALSE, include=FALSE}

# based on suggested grpaphs
X1 <- c("f_property_type * n_accommodates", "f_property_type * f_host_response_time")


# Additional dummies based on graphs suggestion
X2  <- c("f_property_type*d_air_conditioning", 
         "f_property_type*d_elevator",
         "f_property_type*d_dryer",
         "f_property_type*d_washer",
         "f_property_type*d_wifi",
         "f_property_type*d_kitchen", 
         "f_property_type*d_breakfast")


# all dummies with property type
X3  <- c("f_property_type*f_neighbourhood_group_cleansed", "n_accommodates*f_neighbourhood_group_cleansed",
         paste0("(f_property_type) * (",
                paste(dummies, collapse=" + "),")"))

```

```{r message=FALSE, warning=FALSE, include=FALSE}

# Create models in levels models: 1-8
modellev1 <- " ~ n_accommodates"
modellev2 <- paste0(" ~ ",paste(basic_lev,collapse = " + "))
modellev3 <- paste0(" ~ ",paste(c(basic_lev,basic_add,reviews),collapse = " + "))
modellev4 <- paste0(" ~ ",paste(c(basic_lev,basic_add,reviews, poly_lev),collapse = " + "))
modellev5 <- paste0(" ~ ",paste(c(basic_lev,basic_add,reviews,poly_lev, X1),collapse = " + "))
modellev6 <- paste0(" ~ ",paste(c(basic_lev,basic_add,reviews,poly_lev, X1, X2),collapse = " + "))
modellev7 <- paste0(" ~ ",paste(c(basic_lev,basic_add,reviews,poly_lev, X1, X2, dummies),collapse = " + "))
modellev8 <- paste0(" ~ ",paste(c(basic_lev,basic_add,reviews,poly_lev, X1, X2, dummies, X3),collapse = " + "))

```

```{r message=FALSE, warning=FALSE, include=FALSE}
model_table_view <- data.frame(Model = c('M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8'),
                               Predictors = c('Nr of accommodates', 
                               'M1 + number of beds + number of days since first review + property type', 
                               'M2 + Nr bathrooms + Neighbourhood group + host reponse rate + reviews per month + 
                               average review score rating + number of reviews',
                               'M3 + squared termof guests + squared and cubic terms of number of days since first review',
                               'M4 + property type and number of guests interaction + property type and host response time interaction',
                               'M5 + property type interaction with adummies as air conditioning, elavator, 
                               dryer, washer, wifi, kitchen and breakfast',
                               'M6 + all other amenities',
                               'M7 + all other amenities, Neighbourhoods interacted with property type'))


model_table_view %>%
  kbl(caption = "New York Airbnb apartment price prediction Models", booktabs = T) %>%
  kable_classic(full_width = T, html_font = "Cambria")
```

```{r message=FALSE, warning=FALSE, include=FALSE}

# Separate hold-out set #
#----------------------------------------
# create a holdout set (20% of observations)
smp_size <- floor(0.2 * nrow(df))

# Set the random number generator: It will make results reproducable
set.seed(12345)

# A) create ids:
# 1) seq_len: generate regular sequences
# 2) sample: select random rows from a table
holdout_ids <- sample(seq_len(nrow(df)), size = smp_size)
df$holdout <- 0
df$holdout[holdout_ids] <- 1

#Hold-out set Set
data_holdout <- df %>% filter(holdout == 1)

#Working data set
data_work <- df %>% filter(holdout == 0)
```


```{r message=FALSE, warning=FALSE, include=FALSE}

# Utilizing the Working data set:
#   a) estimating measures on the whole working sample (R2,BIC,RMSE)
#   b) Doing K-fold cross validation to get proper Test RMSE

## K = 5
k_folds=5
# Create the folds
set.seed(12345)

folds_i <- sample(rep(1:k_folds, length.out = nrow(data_work) ))
# Create results
model_results_cv <- list()


for (i in (1:8)){
  model_name <-  paste0("modellev",i)
  model_pretty_name <- paste0("(",i,")")

  yvar <- "price"
  xvars <- eval(parse(text = model_name))
  formula <- formula(paste0(yvar,xvars))

  # Initialize values
  rmse_train <- c()
  rmse_test <- c()

  model_work_data <- lm(formula,data = data_work)
  BIC <- BIC(model_work_data)
  nvars <- model_work_data$rank -1
  r2 <- summary(model_work_data)$r.squared

  # Do the k-fold estimation
  for (k in 1:k_folds) {
    test_i <- which(folds_i == k)
    # Train sample: all except test_i
    data_train <- data_work[-test_i, ]
    # Test sample
    data_test <- data_work[test_i, ]
    # Estimation and prediction
    model <- lm(formula,data = data_train)
    prediction_train <- predict(model, newdata = data_train)
    prediction_test <- predict(model, newdata = data_test)

    # Criteria evaluation
    rmse_train[k] <- mse_lev(prediction_train, data_train[,yvar] %>% pull)**(1/2)
    rmse_test[k] <- mse_lev(prediction_test, data_test[,yvar] %>% pull)**(1/2)

  }

  model_results_cv[[model_name]] <- list(yvar=yvar,xvars=xvars,formula=formula,model_work_data=model_work_data,
                                         rmse_train = rmse_train,rmse_test = rmse_test,BIC = BIC,
                                         model_name = model_pretty_name, nvars = nvars, r2 = r2)
}

model <- lm(formula,data = data_train)
prediction_train <- predict(model, newdata = data_train)
prediction_test <- predict(model, newdata = data_test)

#skim(data_train$ln_days_since)

t1 <- imap(model_results_cv,  ~{
  as.data.frame(.x[c("rmse_test", "rmse_train")]) %>%
    dplyr::summarise_all(.funs = mean) %>%
    mutate("model_name" = .y , "model_pretty_name" = .x[["model_name"]] ,
           "nvars" = .x[["nvars"]], "r2" = .x[["r2"]], "BIC" = .x[["BIC"]])
}) %>%
  bind_rows()
t1
column_names <- c("Model", "N predictors", "R-squared", "BIC", "Training RMSE",
                 "Test RMSE")

# R2, BIC on full work data-n.
# In sample rmse: average on training data; avg test : average on test data

OLS_models <- t1 %>%
  select("model_pretty_name", "nvars", "r2" , "BIC", "rmse_train", "rmse_test")
colnames(OLS_models) <- column_names
print(xtable(OLS_models, type = "latex", digits=c(0,0,0,2,0,2,2)), file = paste0(output, "OLS_models.tex"),
      include.rownames=FALSE, booktabs=TRUE, floating = FALSE)

# RMSE training vs test graph
t1_levels <- t1 %>%
  dplyr::select("nvars", "rmse_train", "rmse_test") %>%
  gather(var,value, rmse_train:rmse_test) %>%
  mutate(nvars2=nvars+1) %>%
  mutate(var = factor(var, levels = c("rmse_train", "rmse_test"),
                      labels = c("RMSE Training","RMSE Test")))

model_result_plot_levels <- ggplot(data = t1_levels,
                                   aes(x = factor(nvars2), y = value, color=factor(var), group = var)) +
  geom_line(size=1,show.legend=FALSE, na.rm = TRUE) +
  scale_color_manual(name="",
                     values=c("#fde725","#440154")) +
  geom_dl(aes(label = var),  method = list("last.points", dl.trans(x=x-1), cex=0.4)) +
  theme_bw()
model_result_plot_levels

save_fig("fig14-model-result-plot-levels", output, "small")
# Model 7 gives the best result based on the RMSE result
```


```{r message=FALSE, warning=FALSE, include=FALSE}

# Set lasso tuning parameters:
#----------------------------------------------------
# a) basic setup
train_control <- trainControl( method = "cv", number = k_folds)
# b) tell the actual lambda (penalty parameter) to use for lasso
tune_grid     <- expand.grid("alpha" = c(1), "lambda" = seq(0.05, 1, by = 0.05))
# c) create a formula
# creating two predictors to be used for OLS, LASSO, Random forest, and GBM
predictors_1 <- c(basic_lev, basic_add, reviews, dummies)
predictors_2 <- c(basic_lev,basic_add,reviews,poly_lev, X1, X2, dummies) #Model 7, best model based on CV RMSE

```


```{r message=FALSE, warning=FALSE, include=FALSE}
# OLS BASIC           
#################################

# Using OLS for the Basic variables
set.seed(12345)
system.time({
  ols_model <- train(
    formula(paste0("price ~", paste0(predictors_2, collapse = " + "))),
    data = data_work,
    method = "lm",
    trControl = train_control
  )
})

ols_model_coeffs <-  ols_model$finalModel$coefficients
ols_model_coeffs_df <- data.frame(
  "variable" = names(ols_model_coeffs),
  "ols_coefficient" = ols_model_coeffs
) %>%
  mutate(variable = gsub("`","",variable))


```



```{r message=FALSE, warning=FALSE, include=FALSE}
# LASSO               
# -------------------------------------------------
# setting seed
set.seed(12345)
system.time({
  lasso_model <- caret::train(
    formula(paste0("price ~", paste0(predictors_2, collapse = " + "))),
    data = data_work,
    method = "glmnet",
    tuneGrid =  expand.grid("alpha" = 1, "lambda" = seq(0.01, 0.25, by = 0.01)),
    preProcess = c("center", "scale"),
    trControl = train_control
  )
})

print(lasso_model$bestTune$lambda)

lasso_coeffs <- coef(
  lasso_model$finalModel,
  lasso_model$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>% 
  rename(coefficient = `s1`)  # the column has a name "1", to be renamed

print(lasso_coeffs)

lasso_coeffs_nz <- lasso_coeffs %>%
  filter(coefficient!=0)
print(nrow(lasso_coeffs_nz))

# Evaluate model. CV error:
lasso_cv_rmse <- lasso_model$results %>%
  filter(lambda == lasso_model$bestTune$lambda) %>%
  dplyr::select(RMSE)
print(lasso_cv_rmse[1, 1])

regression_coeffs <- merge(ols_model_coeffs_df, lasso_coeffs_nz, by = "variable", all=TRUE)
regression_coeffs %>%
  write.csv(file = paste0(output, "regression_coeffs.csv"))

```



```{r message=FALSE, warning=FALSE, include=FALSE}
# CART        
#-------------------------------------------
# setting seed
set.seed(12345)
system.time({
  cart_model <- train(
    formula(paste0("price ~", paste0(predictors_1, collapse = " + "))),
    data = data_work,
    method = "rpart",
    tuneLength = 10,
    trControl = train_control
  )
})


fancyRpartPlot(cart_model$finalModel, sub = "", palettes = "Purples")

```



```{r message=FALSE, warning=FALSE, include=FALSE}
# RANDOM FOREST     
#-------------------------------------------
# using all variables without their functional forms. Using predictor 1
# setting seed
# set tuning
tune_grid <- expand.grid(
  .mtry = c(8),
  .splitrule = "variance",
  .min.node.size = c(50)
)

# set seed
set.seed(12345)
system.time({
rf_model <- train(
  formula(paste0("price ~", paste0(predictors_1, collapse = " + "))),
  data = data_train,
  method = "ranger",
  trControl = train_control,
  tuneGrid = tune_grid,
  importance = "impurity"
)
})
rf_model
```



```{r message=FALSE, warning=FALSE, include=FALSE}

# GBM              
#----------------------
# Basic GMB model
gbm_grid <-  expand.grid(interaction.depth = c(5, 10), # complexity of the tree
                         n.trees = 250, # Number of trees
                         shrinkage = 0.1, # learning rate: how quickly the algorithm adapts
                         n.minobsinnode = 20 # the minimum number of training set samples in a node to commence splitting
)


set.seed(12345)
system.time({
  gbm_model <- train(formula(paste0("price ~", paste0(predictors_1, collapse = " + "))),
                     data = data_work,
                     method = "gbm",
                     trControl = train_control,
                     verbose = FALSE,
                     tuneGrid = gbm_grid)
})
gbm_model



```


```{r message=FALSE, warning=FALSE, include=FALSE}
# Turning parameter choice 1
result_1 <- matrix(c(
  rf_model$finalModel$mtry,
  rf_model$finalModel$min.node.size
),
nrow=1, ncol=2,
dimnames = list("Model A",
                c("Min vars","Min nodes"))
)
kable(x = result_1, format = "latex", digits = 3) %>%
  cat(.,file= paste0(output,"rf_models_turning_choices.tex"))
```

```{r message=FALSE, warning=FALSE, include=FALSE}
saveRDS(ols_model, paste0(data_out, 'OLS.rds'))
saveRDS(lasso_model, paste0(data_out, 'lasso.rds'))
saveRDS(cart_model, paste0(data_out, 'cart.rds'))
saveRDS(rf_model, paste0(data_out,'random_forest.rds'))
saveRDS(gbm_model, paste0(data_out,'gbm.rds'))
```


```{r message=FALSE, warning=FALSE, include=FALSE}
# FINAL MODELS         
#--------------------------
final_models <-
  list("OLS" = ols_model,
       "LASSO" = lasso_model,
       "CART" = cart_model,
       "Random forest"= rf_model,
       "GBM"  = gbm_model
       )

results <- resamples(final_models) %>% summary()

# Save output --------------------------------------------------------
# Model selection is carried out on this CV RMSE
```

```{r message=FALSE, warning=FALSE, include=FALSE}
# Evaluating both data sets R squared
result_3r <- imap(final_models, ~{
  mean(results$values[[paste0(.y,"~Rsquared")]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("CV Rsquared" = ".")

result_3r
```


```{r message=FALSE, warning=FALSE, include=FALSE}
# Evaluating both data sets
result_4 <- imap(final_models, ~{
  mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("CV RMSE" = ".")

result_4

kable(x = result_4, format = "latex", digits = 3, booktabs=TRUE, linesep = "") %>%
  cat(.,file= paste0(output,"final_models_cv_rmse.tex"))



# evaluate preferred model on the holdout set -----------------------------

result_5 <- map(final_models, ~{
  RMSE(predict(.x, newdata = data_holdout), data_holdout[["price"]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("Holdout RMSE" = ".")

result_5

kable(x = result_5, format = "latex", digits = 3, booktabs=TRUE, linesep = "") %>%
  cat(.,file= paste0(output,"final_models_houldout_rmse.tex"))

```


```{r message=FALSE, warning=FALSE, include=FALSE}
# Diagnsotics
#-------------------------------------------------

# Variable Importance Plots 
#-------------------------------------------------
# first need a function to calculate grouped varimp
group.importance <- function(rf.obj, groups) {
  var.imp <- as.matrix(sapply(groups, function(g) {
    sum(importance(rf.obj)[g], na.rm = TRUE)
  }))
  colnames(var.imp) <- "MeanDecreaseGini"
  return(var.imp)
}


rf_model_var_imp <- importance(rf_model$finalModel)/1000
rf_model_var_imp_df <-
  data.frame(varname = names(rf_model_var_imp),imp = rf_model_var_imp) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))

```

```{r message=FALSE, warning=FALSE, include=FALSE}

# full varimp plot, top 10 only

rf_model_var_imp_plot_b <- ggplot(rf_model_var_imp_df[1:10,], aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color ="#440154", size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color="#440154", size=0.75, alpha = 0.6) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw() +
  theme(axis.text.x = element_text(size=6), axis.text.y = element_text(size=6),
        axis.title.x = element_text(size=6), axis.title.y = element_text(size=6))
rf_model_var_imp_plot_b

save_fig("fig15-rf-var-imp", output, "small")
```

```{r message=FALSE, warning=FALSE, include=FALSE}

# 2) varimp plot grouped
#---------------------------------------------
# grouped variable importance - keep binaries created off factors together

varnames <- rf_model$finalModel$xNames
f_neighbourhood_group_cleansed_varnames <- grep("f_neighbourhood_group_cleansed",varnames, value = TRUE)
f_property_type_varnames <- grep("f_property_type",varnames, value = TRUE)

groups <- list(f_neighbourhood_group_cleansed=f_neighbourhood_group_cleansed_varnames,
               f_property_type = f_property_type_varnames,
               f_bathroom = "f_bathroom",
               n_days_since = "n_days_since",
               n_accommodates = "n_accommodates",
               n_beds = "n_beds")

rf_model_var_imp_grouped <- group.importance(rf_model$finalModel, groups)
rf_model_var_imp_grouped_df <- data.frame(varname = rownames(rf_model_var_imp_grouped),
                                            imp = rf_model_var_imp_grouped[,1])  %>%
  mutate(imp_percentage = imp/sum(imp))

rf_model_var_imp_grouped_plot <-
  ggplot(rf_model_var_imp_grouped_df, aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color="#440154", size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color="#440154", size=0.6) +
  ylab("Importance (Percent)") +   xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1), breaks = seq(0, 0.3, 0.01)) +
  theme_bw() +
  theme(axis.text.x = element_text(size=6), axis.text.y = element_text(size=6),
        axis.title.x = element_text(size=6), axis.title.y = element_text(size=6))
rf_model_var_imp_grouped_plot

save_fig("fig16-rf-var-imp-group", output, "small")

```



```{r message=FALSE, warning=FALSE, include=FALSE}

# Partial Dependence Plots 
# ----------------------------------------------------------------
# Number of accommodates
pdp_n_acc <- pdp::partial(rf_model, pred.var = "n_accommodates", pred.grid = distinct_(data_holdout, "n_accommodates"), train = data_train)
pdp_n_acc_plot <- pdp_n_acc %>%
  autoplot( ) +
  geom_point(color="#440154", size=3) +
  geom_line(color="#440154", size=1) +
  ylab("Predicted price") +
  xlab("Accommodates (persons)") +
  scale_x_continuous(limit=c(1,7), breaks=seq(1,7,1))+
  theme_bw()
pdp_n_acc_plot

save_fig("fig17-pdp-n-accom", output, "small")
```

```{r message=FALSE, warning=FALSE, include=FALSE}
# Partial Dependence Plots 
# Property type
pdp_n_propertytype <- pdp::partial(rf_model, pred.var = "f_property_type", pred.grid = distinct_(data_holdout, "f_property_type"), train = data_train)
pdp_n_propertytype_plot <- pdp_n_propertytype %>%
  autoplot( ) +
  geom_point(color="#440154", size=4) +
  ylab("Predicted price") +
  xlab("Property type") +
  theme_bw()
pdp_n_propertytype_plot

save_fig("fig17-pdp-n-propertytype", output, "small")

```

```{r message=FALSE, warning=FALSE, include=FALSE}

# Subsample performance: RMSE / mean(y) ---------------------------------------

data_holdout_w_prediction <- data_holdout %>%
  mutate(predicted_price = predict(rf_model, newdata = data_holdout))



######### create nice summary table of heterogeneity
a <- data_holdout_w_prediction %>%
  mutate(is_low_size = ifelse(n_accommodates <= 3, "small apt", "large apt")) %>%
  group_by(is_low_size) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price),
    mean_price = mean(price),
    rmse_norm = RMSE(predicted_price, price) / mean(price)
  )


b <- data_holdout_w_prediction %>%
  group_by(f_neighbourhood_group_cleansed) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price),
    mean_price = mean(price),
    rmse_norm = rmse / mean_price
  )

c <- data_holdout_w_prediction %>%
  group_by(f_property_type) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price),
    mean_price = mean(price),
    rmse_norm = rmse / mean_price
  )


d <- data_holdout_w_prediction %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price),
    mean_price = mean(price),
    rmse_norm = RMSE(predicted_price, price) / mean(price)
  )

# Save output
colnames(a) <- c("", "RMSE", "Mean price", "RMSE/price")
colnames(b) <- c("", "RMSE", "Mean price", "RMSE/price")
colnames(c) <- c("", "RMSE", "Mean price", "RMSE/price")
d<- cbind("All", d)
colnames(d) <- c("", "RMSE", "Mean price", "RMSE/price")

line1 <- c("Type", "", "", "")
line2 <- c("Apartment size", "", "", "")
line3 <- c("Neighbourhood", "", "", "")

result_3 <- rbind(line2, a, line1, c, line3, b, d) %>%
  transform(RMSE = as.numeric(RMSE), `Mean price` = as.numeric(`Mean price`),
            `RMSE/price` = as.numeric(`RMSE/price`))


result_3

options(knitr.kable.NA = '')
kable(x = result_3, format = "latex", booktabs=TRUE, linesep = "",digits = c(0,2,1,2), col.names = c("","RMSE","Mean price","RMSE/price")) %>%
  cat(.,file= paste0(output, "performance_across_subsamples.tex"))
options(knitr.kable.NA = NULL)


```

```{r message=FALSE, warning=FALSE, include=FALSE}

# FIGURES FOR FITTED VS ACTUAL OUTCOME VARIABLES #
##--------------------------------------------------

Ylev <- data_holdout[["price"]]

# Predicted values
prediction_holdout_pred <- as.data.frame(predict(gbm_model, newdata = data_holdout, interval="predict")) 

predictionlev_holdout <- cbind(data_holdout[,c("price","n_accommodates")],
                               prediction_holdout_pred)



# Create data frame with the real and predicted values
d <- data.frame(ylev=Ylev, predlev=predictionlev_holdout[,3] )
# Check the differences
d$elev <- d$ylev - d$predlev

# Plot predicted vs price
level_vs_pred <- ggplot(data = d) +
  geom_point(aes(y=ylev, x=predlev), color = "#440154", size = 1,
             shape = 16, alpha = 0.5, show.legend=FALSE, na.rm=TRUE) +
  geom_segment(aes(x = 0, y = 0, xend = 350, yend =350), size=0.8, color="black", linetype=2) +
  labs(y = "Price (US dollars)", x = "Predicted price  (US dollars)") +
  theme_bw() 
level_vs_pred

save_fig("fig17-level-vs-pred", output, "small")

```



## Introduction 

The aim of this report is to provide detailed description for the price prediction model. The main goal of this project is to help the a company to set price for their new apartments which are not yet in the market.To build a price prediction model for a company which is operating small and mid-size apartments hosting from two to six guests in the New York, the data is taken from Inside Airbnb which can be found [here](http://insideairbnb.com/new-york-city/). As a result of data cleaning, munging, and analysis, five price prediction models, OLS, Lasso, Cart, Random Forest and GBM, Gradient Boosting Machine, are created. As a result GBM showed the best prediction result with 65.38 USD RMSE. The major predictor features are the number of accommodates, number of bathrooms, the number of beds, neighborhood, and amenities such as availability of washer, gym, elevator are the most important. Other characteristics such as days since the first review is also an important predictor. Eventually the final goal of the project is to finalize a better prediction model measured in relative RMSE values. 


## Cleaning and Preprationg Data

The data for the project is taken from the [Inside Airbnb](http://insideairbnb.com/new-york-city/) website. The original data set has a single data table which contains 38186 observations and 74 columns. Some of the important columns are as following: ID, price, number of accommodates, property type, room type, amenities and other host and rental unit characteristics. The data refer to the one night rental prices between January 6 to January 9, 2022. The target variable is price per night per person in US dollars. The original data set contains significant amount of information which requires data cleaning because it is easier to work with Tidy data tables. Initially, I dropped columns such as, host URL, host picture URL, house rules, notes, host location and others.Because these columns are not target of this project. Moreover, To transform the original data to tidy data table, the major part of transformation consists of processing _amenities_ column to binary variables. After the preliminary transformation of amenities to binaries, the total number of columns resulted to 3177, from which 3134 are different types of amenities. The code for this transformation is as following. further codes can be found in the [1_cleaning_preparing](https://github.com/ghazalayobi/DA3/blob/main/A2/codes/1_cleaning_preparing.R) section. 

```{r eval=FALSE}
#define levels and dummies 
levs <- levels(factor(unlist(df$amenities)))
df<-cbind(df,as.data.frame(do.call(rbind, lapply(lapply(df$amenities, factor, levs), table))))
```


As a result of the above code, meaningful grouping is required to combine similar amenities to one binary variable group. For example, the data contained information about different types of TVs. Such as HDTV, 18 inches HDTV, 32 inches HDTV and many others. To narrow variable grouping, only binaries between 1% to 99% values are kept in the data set. After the grouping of similar amenities the total number of variables decreased to 133 containing 90 amenities. The codes is as following

```{r eval=FALSE}
# function to merge columns with the same key word in them
for (i in column_names) {
  xdf <- amts %>% select(matches(i))
  
  amts$new_col <- ifelse(rowSums(xdf)>0, 1, 0)
  
  names(amts)[names(amts) == "new_col"] <- paste0("have_", i)
  
  amts <- amts %>% select(-colnames(xdf)) 
  
} 

# keep only columns where the percentage of 1s is at least 1% and at most 99%
selected <- sapply(names(amts), function(x){
  ratio <- sum(amts[[x]])/nrow(amts)*100
  if (between(ratio, 1, 99)) {
    return(TRUE)
  } else { return(FALSE) }
})

# taking only selected or grouped columns
amenities <- amts[,selected]
```

For detailed description of data cleaning please find the codes [here](https://github.com/ghazalayobi/DA3/blob/main/A2/codes/1_cleaning_preparing.R). It is worth mentioning that having right structured data, proper ID variable are the essential while working with this big data sets. Moreover, after the preliminary stage of cleaning, data preparation comes next. Preparing data for analysis consists of the following steps: identifying types of the variables, removing duplicates, and addressing missing values. The selected data set, Airbnb New York, consists of numerical values such as price, factor variables such as property type and neighborhood, and binary variables such as all amenities and host characteristics. Other data types are first review and the date scraped. I identify the data types such as price as numeric value and factor variables are created for the predictors such as, property types and neighborhood types. For this process, I used data summaries which can be found in the appendix section. 


**Filters** The main goal of the project is to predict prices of apartments which accommodates between 2 to 6 guests, thus the data was filtered accordingly. The key variable, price, contained extreme value and missing observations. To narrow down the project goal the target variable, price per night included extreme values above 900 USD per night which contained of less than 1% of the observations, thus, price was filtered to less than 500 USD and dropping the observation where price is missing. Moreover, As the goal of project is to build price prediction model, it is crucial to check price and log of price distribution. I created of price and log of price distributions are as following:

```{r echo=FALSE, message=FALSE, warning=FALSE, out.width='50%', fig.height=3.5}
hist_price
hist_ln_price

```


Price distribution shows Airbnb  apartment prices is skewed with a long right tail and the log price is close to normally distributed. In this project, log of price is not considered, prediction is carried out with price for all of the models. 

In addition as this project's main goal is to build price prediction for small and mid-size apartments, thus, the following categories for the property types is filtered for the analysis which are as following categories: entire home or apartment, entire serviced apartment, entire condominium, entire loft, entire rental unit. Based on the dictionary definition, a [loft](https://www.apartments.com/blog/what-is-a-loft-apartment), [condominium](https://www.bankrate.com/real-estate/condo-vs-apartment/#:~:text=The%20biggest%20difference%20between%20a,are%20owned%20instead%20of%20rented.) and a [rental unit](https://www.lawinsider.com/dictionary/rental-unit) refer to different types of apartments. The below figures show the number of guests, property type, room type, neighborhood groups, number of bathrooms and number of beds along with their mean prices. It is shown that there is one room type across all data set. 


```{r echo=FALSE, message=FALSE, warning=FALSE, out.width='50%', fig.height=4}
jitter_accomm
jitter_property_type
jitter_room_type
jitter_neighbourhood_group
jitter_bathrooms
jitter_beds
```


**Factoring** I created factor variables such as, property type, neighborhood groups based on the above plots.

**New Variables** the second step in the process is creating new meaningful variables such as number of days since the first review, which is subtracting date of first review from the date when the data was scraped and log, square and cubic functional forms of number of days since first review were created. 


**Grouping Numeric Variables** some of the numeric variables such as number of bathrooms. This variable contained information such as half bathroom. I created another variable as a factor of bathrooms with four cuts of 0, 1, 2, and 10. This means to add all the variables in the mentioned groups. Another example is number of reviews for 0, 1-51 and 51 above. 

After the key filters and grouping variables there were 15 variables with missing values. Missing values were addressed as following: first assumption is there is at least one bathroom in each apartment, second assumption is if the number of guests are less than 4 then impute 1, otherwise imputing 2. Missing number of beds were replace with half of number of accommodates, assuming there are double beds in the apartments. It is assumed that the minimum number of nights is one and minimum number of reviews is also 1. Flags were  created to indicate the missing values in the each predictor.


```{r message=FALSE, warning=FALSE, include=FALSE}
count_rows <- count(df)
count_cols <- ncol(df)
```



As a result of data cleaning, and preparation the total number of observations are `r count_rows` with `r count_cols` columns.The cleaned data is saved to this [repository](https://raw.githubusercontent.com/ghazalayobi/DA3/main/A2/data/clean/airbnb_ny_cleaned.csv). 

# Data Analysis and Feature Engineering 

The next most crucial step in this project is data analysis which is conducted in the following steps. First and far most importantly I defined and grouped variables as following.

Feature engineering includes what type of predictor variables to include, and deciding about functional forms of predictors and possible interactions. The data is grouped as following:

- **Basic variables** which consists of the main predictors such as: number of accommodates, property types, number of beds, number of days since the first review and flag variable of number of days since the first review to indicate missing. As the focus of project is on small and mid-size apartment thus, property types are as following: Entire home or apartment, serviced apartment, Condominium, and entire rental unit. 

- **Basic addition** this includes key factorized variables, such as, neighborhoods groups, and host response time.

- **Review Variables** consists of the crucial guests reviews predictors such as total number of reviews, number of reviews per month and host review score rating and reviews flags which shows missing variables.

- **Polynomial level** consists of squared terms of guests and squared and cubic terms for days since the first review.

- **Amenities dummies** which consisted of the binary values for all of amenities. 

The next step in the process is finding the right interactions. The below plots were used to see prices changes across each interaction. 

```{r echo=FALSE, message=FALSE, warning=FALSE, Echo=FALSE}
g_interactions1
```


Based of the above plot I created three categories of interactions which are as following:

- **X1** property type times number of accommodates and property type times host response time (categorical variables)

- **X2** property type time air conditioning, elevator, dryer washer, wife, kitchen and breakfast dummy variables

- **X3** property type time neighbouhood groups, and all amenities 

## Modeling 

**Regressions** The best model gives the best prediction in the live data. Before turning to the modeling part of the project, it is worth mentioning that in order to avoid over fitting, the original data is split into two random parts by 20% to 80% ratio. Holdout set contains the 20% and the rest is work data set. In addition the k-fold cross-validation is a good way to find a model which gives the best prediction for the original data. For the purpose of this project 5-fold cross validation is used. This means splitting the data into five random samples and calculating and deciding based on the average of 5 CV RMSE result. Eight basic OLS regression models from simplest to the most complex one were used to find a better model to use for further analysis. Below are the list of models, 


```{r echo=FALSE, message=FALSE, warning=FALSE}

model_table_view %>%
  kbl(caption = "New York Airbnb apartment price prediction Models") %>%
  kable_classic(full_width = T, html_font = "Cambria")
```


5-fold cross-validation RMSE suggests that Model 7 regression has a better performance and it has the lowest RMSE value. The table is as following. 


```{r echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
OLS_models  %>%
  kbl(caption = "Models Evaluation") %>%
  kable_minimal(full_width = F, html_font = "Cambria")
```

```{r echo=FALSE, message=FALSE, warning=FALSE, out.width='50%', fig.height=4, fig.align='center'}
model_result_plot_levels

```


## Models

It is important to run and evaluate different models for a given data set. In order to predict apartment prices, the following models and algorithms were used. Naming them as following:

- **OLS** and **LASSO** using model 7 based CV-RMSE result

- **CART**, **Random Forest** , **GBM**  using basic level variables, basic additions, review variables and amenities as dummy variables.

**Setting LASSO tuning parameter** Before runing the models is sat the lasso tuning parameter, it servers as a weight for the penalty term versus OLS fit. As a result it derives the strength of the variables selection. Lamda, value for tuning parameter is set between 0.05 and 1. 

### OLS Model:
OLS model is the fastest among all models which has CR RMSE of 67.99 and R-squared of 31.67% and with 107 predictors.For this purpose I model 7 which has basic variables, basic additions, reviews, polynomials and a dew interactions

### LASSO:

LASSO is the most widely used shrinkage method. It is an algorithm that fits a model by shrinking coefficients, some them to zero by adding a penalty term. After running 5-fold cross validation for the selecting the optimal value for lambda. In the end LASSO picked 149 predictors out 150 with the CR-RMSE of 67.97247. For this method, I used the same Model 7 with few interactions but it does not have as manay interactions as Model 8. 

### CART

Regression trees are called CART. It is a building and growing a tree. This algorithm has no formula, the goal is to arrive at a set of bins of predictors. This algorithm splits the bins into smaller bins. For CART algorithm no functional or interactions were given. The variables were basic variables, basic additions, reviews and dummies with cp values of 0.005054542. CART selected 104 predictors. CART result is as following

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}

fancyRpartPlot(cart_model$finalModel, sub = "", palettes = "Purples")

```

### Random Forest

Random forest is an example of ensemble method which uses the results of many predictive models and combine those results to generate a final prediction. I used basic variables, basic additions, reviews and dummies for this model the minimum nod size of 50. The CR RMSE is 67.06593. 

### GBM

The last model for this case study is GBM. It gives the best result across all models in terms of relative RMSE. GBM basic tuning model is referred as black box because there is no more regression trees. The complexity of tree in this GBM model is 5 and 10 which RMSE of 65.75680, and 65.38265 respectively. The Rsquared is 36.8%. 


**Result** Based on the below table of models, it is can be seen that GBM model has the best performance. 5-fold cross validation RMSE for the data is 65.38 USD RMSE which is 1.683 USD RMSE less than the second best model which is Random Forest. Moreover, The 5-fold cross validation RMSE for the GBM model using Holdout set is 64.38 USD RMSE indicating a better performance than other models illustrating a better performance from second best model of random forest by 1.671 USD RMSE. GBM model tend to be robust, thus, the selected model for this project is GBM BASIC TUNING model.

```{r echo=FALSE, message=FALSE, warning=FALSE}
final_result4_5 <- cbind(result_4, result_5, result_3r)

final_result4_5 %>% 
  kbl() %>% 
  kable_minimal(full_width = F, html_font = "Cambria")
```


## Diagnostics

The best selected model, GBM, is an ensemble method which is a black box model, because it does not reveal the pattern of association that drive prediction. However, diagnostic tools can be used to uncover information about the patterns of association which drive prediction. Some of them are as following: 


**Variable Importance plot** it shows the average importance of fit when we use an x variable or group of x variables. Variable importance plot for Top 10 important variables shows that number of bathrooms, number of accommodates, Manhattan neighborhood are the most important along with amenities such as washer, and gym. The grouped variable importance shows that bathrooms, neighborhoods, number of accommodates are the most important variables. 


```{r echo=FALSE, message=FALSE, warning=FALSE, out.width='50%', fig.height=4}

# ploting the variable importance plot

rf_model_var_imp_plot_b
rf_model_var_imp_grouped_plot
```


**Partial Dependence Plot** it shows how average y differs for different values of x conditional on all other predictor variables. Partial dependence plot is based on predictors for the holdout set.  Partial dependence plot for number of accommodates and price shows that price increases as the number of accommodates. 



```{r echo=FALSE, message=FALSE, warning=FALSE, out.width='50%', fig.height=4}

pdp_n_acc_plot
pdp_n_propertytype_plot

```


**Performance Across Subsamples**

Examining the fit in the various sub samples can inform us about external validity of the prediction. 

```{r echo=FALSE, message=FALSE, warning=FALSE}

result_3 %>% 
  kbl() %>% 
  kable_minimal(full_width = F, html_font = "Cambria")

```


**Actual vs Predicted Price** another post prediction diagnostics is comparing the predicted prices versus the actual prices. The figure below shows that prediction does a better job for lower than higher prices. 

```{r echo=FALSE, message=FALSE, warning=FALSE, out.width='50%', fig.height=4,fig.align='center'}

level_vs_pred
```


# Compraring results with case study:

In the case study of Gabors book for Data Analysis one of the main predictors is different categories of room type, however in this case study of New York, there was only one type of room. 

The purpose of the case study was to build a model to predict property rental prices in london. In the case study as a result of running different models GBM won the contest in terms of best fit. It just marginally beating random forest. Based on the case study result Random Forest worked well and faster and importantly random forest is relatively easy to implement. In this case study to build a prediction model for New York prices, GBM outperformed however models, same as the case study in the book Random Forest was the second best for New York.

In the case of London Airbnb prices the top most important variables are number of accommodates, room type, neighbourhood and beds. In the case study of New York the most important variables are: number of bathrooms and accomodates, and other amenities such as washer, gym and elvator. 

## Conclusion
The goal of this report was to find a better model to predict Airbnb prices in New York for a small to mid-size apartments. Five models were illustrated to compare and contrast across models performance. GBM resulted to be the best model by 65.38 USD RMSE, besides this GBM model also shows better performance in holdout set with 64.38 USD RMSE. The second best model was basic Random Forest which has highlights meaningful characteristics about the nature of Airbnb apartments in New York. Key price drivers based on post prediction diagnostics are the number of bathroom, number of accommodates, and availability of amenities such as washer and gym. partial dependence plot also illustrated that the model better predicts Manhattan neighborhood. 

## Notes:
The codes for the regressions are taken from seminars of Data Analysis 3, Gabors book of Data Analysis for Business, Economics and Policy. Moreover, I would like to source that amenities transformation are taken from this GitHub [repository](https://github.com/Viki-Meszaros/CEU-Data-Analysis-3/blob/main/Assignment_1/Codes/1_data_cleaning.R). However, I further added small changes to the codes. 


# Appendix

```{r echo=FALSE, message=FALSE, warning=FALSE, Echo=FALSE}
# more interactions 
g_interactions2
```

\newpage


```{r message=FALSE, warning=FALSE, echo=FALSE, show.fig='hold', fig.align='center'}
sum_accomm
# Number of Beds Summary
sum_beds
# Number of Bedrooms Summary
sum_bedrooms 
# Room type Summary
sum_room_type
# Property Type Summary
sum_property_type 
# Host response time Summary
sum_host_res_time 
# Minimum Nights summary
sum_nnights 
# Neighborhood group summary
sum_neighbourhood_group 
```

